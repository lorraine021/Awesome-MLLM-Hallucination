# Awesome-MLLM-Hallucination
This repository collects research on the hallucination problem of Multimodal Large Language Model(MLLM), including their papers and codes/datasets (TODO).  
The main aspects involved are Surveys, Hallucination Evaluation methods (Benchmarks), Hallucination Mitigation methods and some intersting papers that are not directly related to the current topic. Since some of the papers are relatively new and cannot be sure whether they have been included in the specific conferences, they are currently only marked according to the conference acceptance status of the articles that Google Scholar can find.  
If you find some interesting papers not included, please feel free to contact me. We will continue to update this repository!


__`data.`__: data improvement &emsp; | &emsp; __`vis.`__: vision enhancement &emsp; | &emsp;
__`align.`__: multimodal alignment &emsp; | &emsp; __`dec.`__: decoding optimization &emsp; |
__`post.`__: post-process &emsp; | &emsp; __`ben.`__: benchmark  

:large_blue_diamond: citation >= 20 &emsp; | &emsp; :star: citation >= 50 &emsp; | &emsp; :fire: citation >= 100

## Contents  
1. [Surveys](#Surveys)
2. [Hallucination Evaluation methods (Benchmarks)](#Hallucination-Evaluation-methods)
3. [Hallucination Mitigation methods](#Hallucination-Mitigation-methods)
4. [Other](#Other)
   
## Papers
### Surveys
1. A Survey on Hallucination in Large Vision-Language Models, arxiv, 2024, Liu, Hanchao, et al.[[PDF](https://arxiv.org/pdf/2402.00253.pdf)]
2. A Survey of Hallucination in “Large” Foundation Models, arxiv, 2024, Rawte, et al. [[PDF](https://arxiv.org/pdf/2309.05922.pdf)] :star:

### Hallucination Evaluation methods (Benchmarks)
Most work products fine-tuning using their benchmark dataset, which reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. And some papers have designed clever ways to construct such datasets.
1. MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models, arxiv, 2023, Fu, Chaoyou, et al.[[PDF](https://arxiv.org/pdf/2306.13394.pdf)][[Code/Data](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)]   [__`ben.`__:MME] :fire: 
2. Evaluating Object Hallucination in Large Vision-Language Models, EMNLP 2023, Li, Yifan, et al.[[PDF](https://arxiv.org/pdf/2305.10355.pdf)] [__`ben.`__:POPE] :fire: 
3. HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models, arxiv, 2023, Guan, Tianrui, et al.[[PDF](https://www.researchgate.net/profile/Fuxiao-Liu-2/publication/376072740_HALLUSIONBENCH_An_Advanced_Diagnostic_Suite_for_Entangled_Language_Hallucination_Visual_Illusion_in_Large_Vision-Language_Models/links/6568af0e3fa26f66f43abf17/HALLUSIONBENCH-An-Advanced-Diagnostic-Suite-for-Entangled-Language-Hallucination-Visual-Illusion-in-Large-Vision-Language-Models.pdf)][[Code/Data](https://drive.google.com/drive/folders/1C_IA5rx_Hm67TYpdNf3TL5VlM30TLGRQ)]  [__`ben.`__:HALLUSIONBENCH]
4. ALIGNING LARGE MULTIMODAL MODELS WITH FACTUALLY AUGMENTED RLHF, arxiv, 2023, Sun, Zhiqing, et al.[[PDF](https://arxiv.org/pdf/2309.14525.pdf)][[Code/Data](https://llava-rlhf.github.io.)]   [__`ben.`__:MMHAL-BENCH] :large_blue_diamond:
5. MITIGATING HALLUCINATION IN LARGE MULTIMODAL MODELS VIA ROBUST INSTRUCTION TUNING, ICLR 2024, [[PDF](https://openreview.net/pdf?id=J44HfH4JCg)]   [__`ben.`__:GAVIE]:large_blue_diamond:
6. Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites, MMM 2024, Wang, Lei, et al.[[PDF](https://arxiv.org/pdf/2312.01701v1.pdf)][[Code/Data](https://github.com/Anonymousanoy/FOHE)]   [__`ben.`__:FGHE/FOHE(An upgraded version of POPE)]
7. Visual Hallucinations of Multi-modal Large Language Models, arxiv, 2024, Huang, Wen, et al. [[PDF](https://arxiv.org/pdf/2402.14683.pdf)][[Code/Data](https://github.com/wenhuang2000/VHTest)]   [__`ben.`__:two benchmarks generated by VHTest]
8. AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation, arxiv, 2023, Wang, Junyang, et al.[[PDF](https://arxiv.org/pdf/2311.07397v2.pdf)][[Code/Data](https://github.com/junyangwang0410/AMBER)]   [__`ben.`__:AMBER]
9. Eyes wide shut? exploring the visual shortcomings of multimodal llms, arxiv, 2024, Tong, Shengbang, et al.[[PDF](https://arxiv.org/pdf/2401.06209.pdf)]   [__`ben.`__:MMVP]
10. Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models, AAAI-ReLM Workshop. 2024, Lu, Jiaying, et al.[[PDF](https://www.cs.emory.edu/~jyang71/files/lvlm-workshop.pdf)]   [__`ben.`__:MSG-MCQ]
11. Detecting and Preventing Hallucinations in Large Vision Language Models, AAAI 2024, Gunjal, et al.[[PDF](https://arxiv.org/pdf/2308.06394.pdf)][[Code/Data](https://github.com/hendryx-scale/mhal-detect)]   [__`ben.`__:M-HalDetect] :large_blue_diamond:
12. Evaluation and Analysis of Hallucination in Large Vision-Language Models, arxiv, 2023, Wang, Junyang, et al.[[PDF](https://arxiv.org/pdf/2308.15126.pdf)][[Code/Data](https://github.com/junyangwang0410/HaELM)]   [__`ben.`__:HaELM] :large_blue_diamond:
13. 


### Hallucination Mitigation methods 



### Other
1. Hallucination improves the performance of unsupervised visual representation learning, ICCV 2023, [[PDF](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Hallucination_Improves_the_Performance_of_Unsupervised_Visual_Representation_Learning_ICCV_2023_paper.pdf)]  



